{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVh+f5w5J3+6qp8eb0+J5k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishakhun/Google-GenAI-Chatbot/blob/main/Chatbot_Langchain_RAG_Google_Generative_AI_Policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chatbot_Langchain_RAG - Chat with Google Gen AI Policy\n",
        "\n",
        "https://transparency.google/our-policies/privacy-policy-terms-of-service/\n",
        "\n",
        "\n",
        "In this notebook I will cover\n",
        "\n",
        "\n",
        "* Installs, Imports and API Keys\n",
        "* Loading PDFs and chunking with LangChain\n",
        "* Embedding text and storing embeddings\n",
        "* Creating retrieval function\n",
        "* Creating chatbot with chat memory"
      ],
      "metadata": {
        "id": "i6lFsVkjsix7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Installs, Imports and API Keys"
      ],
      "metadata": {
        "id": "fSiNOpWMn4Sd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wl1BNgenVAK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#install necessary libraries\n",
        "\n",
        "!pip install -q langchain==0.0.351 pypdf pandas matplotlib tiktoken textract transformers openai faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "MRQtf8Jnnksi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = input(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYr2btKBoBlm",
        "outputId": "1393f552-264a-4891-add6-d2107bd924ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: 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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Loading Google Generative AI Policies as PDF and chunking with LangChain"
      ],
      "metadata": {
        "id": "SkfQqcZooOHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Convert PDF to text\n",
        "import textract\n",
        "doc = textract.process(\"/content/sample_data/Google Generative AI Policies.pdf\")\n",
        "\n",
        "# Step 2: Save to .txt and reopen\n",
        "with open('Google Generative AI Policies.txt', 'w') as f:\n",
        "    f.write(doc.decode('utf-8'))\n",
        "\n",
        "with open('Google Generative AI Policies.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Step 3: Create function to count tokens\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "# Step 4: Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 24,\n",
        "    length_function = count_tokens,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "jg98nje3oLdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8frF8OpxJG",
        "outputId": "1741ca90-5e4d-484b-cc46-f13862abd3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.documents.base.Document"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  data visualization to ensure chunking was successful\n",
        "\n",
        "# Create a list of token counts\n",
        "token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "# Create a DataFrame from the token counts\n",
        "df = pd.DataFrame({'Token Count': token_counts})\n",
        "\n",
        "# Create a histogram of the token count distribution\n",
        "df.hist(bins=40, )\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0lf9XBkapx0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Embed text and store embeddings"
      ],
      "metadata": {
        "id": "PulZW9jHsE7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embedding model\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create vector database\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "Iu9_BcZup-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Setup retrieval function"
      ],
      "metadata": {
        "id": "pweQ0aSWsNvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check similarity search is working\n",
        "query = \"What is google's commitment to the responsible development of AI?\"\n",
        "docs = db.similarity_search(query)\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "tn3gLq3rqkn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create QA chain to integrate similarity search with user queries (answer query from knowledge base)\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "\n",
        "query = \"What is google's commitment to the responsible development of AI?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "SJcCTe2-rW96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create chatbot with chat memory"
      ],
      "metadata": {
        "id": "Ws-Un9tkroXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Create conversation chain that uses our vectordb as retriver, this also allows for chat history management\n",
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1), db.as_retriever())"
      ],
      "metadata": {
        "id": "UdgaeYbUrc5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "def on_submit(_):\n",
        "    query = input_box.value\n",
        "    input_box.value = \"\"\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Thank you for using Google Gen AI Policy chatbot!\")\n",
        "        return\n",
        "\n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "    display(widgets.HTML(f'<b>User:</b> {query}'))\n",
        "    display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
        "\n",
        "print(\"Welcome to the Google Gen AI Policy Chatbot! Type 'exit' to stop.\")\n",
        "\n",
        "input_box = widgets.Text(placeholder='Please enter your question:')\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ],
      "metadata": {
        "id": "iDcBtfwgrgNU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}